---
layout: post
title:  "Spark 概述"
date:   2018-11-06 18:05:19 +0800
categories: Spark
---

* TOC
{:toc}

# Spark 教程 - 走进 Spark 编程
     

## Spark 概述

在本系列教程中，会对 Spark 在大数据领域中进行介绍。我们将首先会介绍  Apache Spark 编程。然后会介绍一下 Spark 的历史。接着我们会学习为什么 Spark 是被需要的。在之后，我们会全面的讲解 Spark 组件的基础。接下来，我们会学习 Spark 核心抽象和 Spark RDD。最后，我们还会讲解 Spark 的特性，Spark 的限制，和 Spark 的应用场景。

![什么是 Spark？](https://data-flair.training/blogs/wp-content/uploads/sites/2/2018/03/Spark-Tutorial-01.jpg)

## Spark Programming 简介

Spark 是一个通用快速的集群计算框架。换句话说，它是一个开源，用于大范围数据计算的引擎。由于它提供给开发者的 API，可以让数据开发工程师很好的完成需要重复进入数据集中的工作。如在流、机器学习和 SQL 负载查询等。 Spark 可以执行**批量计算和流计算**。批量计算指的是在一个批次中处理之前已经收集好的作业。流计算指的是处理进入 Spark 流中的数据。

Spark 还可以和所有的大数据工具整合。例如可以从 Hadoop 获取数据，也可以运行在 Hadoop 集群上，并把 Hadoop 的 MapReduce 拓展到了更高的层面。而且，在 Spark 中还包含了迭代查询和流处理功能。很多人认为 Spark 是 Hadoop 的拓展，但其实不是这样。Spark 不依赖于 Hadoop 而是单独存在，它有自己的 **Cluster Management** 系统。简单来说，Spark 仅仅用 Hadoop 的存储系统。

Spark 关键之一的特性就是它在集群使用内存计算的能力，这就意味着它进一步增强了处理应用的速度。而且 Spark 还提供了丰富的 API 给用户。如 Java，Scala（Spark 由 Scala 编写），Python 和 R 等。

在和 Hadoop 相比较时，比 Hadoop 的计算能力快 100 倍，在磁盘中获取数据的能力快 10 倍。

## Spark 历史

在 2009 年， UC Berkeley R&D Lab 推出 Apache Spark，这个实验室被称为 AMPLab. 之后在 2010 年，成为了 BSD 协议的开源项目。接着，在 2013 年 spark 被捐赠给 Apache Software Foundation 。在之后的 2014 年，成为了 Apace 维护的顶级项目。

## 为什么选用 Spark ?

![Spark 的特性](https://data-flair.training/blogs/wp-content/uploads/sites/2/2018/03/Why-Spark-01-1.jpg)

一直以来，业内没有通用的计算引擎进行相应的工作，如在这些方面：
1. 执行批量作业，使用 MapReduce.
2. 执行流处理作业，使用 Apache Strom / S4.
3. 执行交互式作业，使用 Apache Impala / Apache Tez.
4. 执行图处理作业，使用 Neo4j / Apache Giraph.

因此一直找不到一个合适的计算引擎，可以同时在实时流处理和批量处理发挥作用。而且存在一个难点是要求计算引擎必须在亚秒级响应并在内存中执行计算。

这时，Spark 来了。它提供了实时流处理、交互式查询、图处理、内存处理以及批量执行的功能。并且有着非常高的处理速度，更易用的标准接口。

## Spark 组件

接下来我们会讨论一下 Spark 的组件，它在数据的快速处理和更简单的开发上给我们带来了希望。Spark 的这些组件解决了使用 Hadoop Mepduce 的一系列问题。

![Spark 组件](https://data-flair.training/blogs/wp-content/uploads/sites/2/2018/02/Apache-Spark-Components.jpg)

### Spark Core

Spark Core 是 Spark 的核心。根本上说，它为所有的 Spark 应用提供了执行平台。

### Spark SQL

Spark SQL 运行在 Spark 之上，可以让用户运行 SQL 和 HQL 查询。通过它我们可以处理结构化和半结构化的内容。在和已经存在的产品相比在运行未修改的查询情况下速度快了 100 倍。

### Spark Streaming 

Sparking Streaming 实现了强有力的交互和问题分析应用。进一步说，数据流是被转换成微批量作业在 spark core 上执行。

### Spark Mlib

包含的机器学习库包含了更有效率和高质量的算法，使其成为数据科学家更好的选择。由于 Spark 在内存处理数据，因此可以极大地迭代算法的性能。

### Spark GraphX

Spark Graphx 是运行在 Spark 之上的图计算引擎，来处理大规模的图数据。

### SparkR

从 R 语言中使用 Apache Spark. 它本身是一个 R 包，并提高了一个轻量级的前端。可以为数据科学家分析大量的数据集。还支持交互运行 jobs 在 R Shell 上。虽然，SparkR 背后的主要思想是探索不同的技术，将 R 的可用性与 Spark 的可扩展性相结合。

## RDD

RDD 是 Spark 的抽象。它是 Resilient Distributed Dataset 的缩写。作为 Spark 数据的基础单元，它代表了跨集群节点的分布式集合，同时提供了并行操作。RDD 本质上是不可变的，虽然它可以通过已经存在的 RDD 生成新的 RDD。

### 创建 RDD 的方式

一般来讲，创建 RDD 有三种方式：

1. Parallelized collections

   通过在 `driver` 中调用 `parallelize` 方法，来创建一个 Parallelized collections.

2. External datasets

   通过调用 `textFile` 方法来创建 RDD。通过读取文件的 URL 并将其作为行集合进行读取。

3. 已经存在的 RDD。

   我们可以对一个已经存在的 RDD 应用转换操作从而形成一个新的 RDD.

### RDD 的操作方式

1. Transformation 操作

   从一个存在的 RDD 中创建一个新的 RDD，在转换过程中将数据集传递给一个函数进行相应操作后返回一个新的数据集。

2. Action 操作

   Action 返回最终的结果给 `driver` 程序或写入外部的数据库。

### RDD 的特性

![RDD 的特性](https://data-flair.training/blogs/wp-content/uploads/sites/2/2018/02/Features-of-RDD-01-1.jpg)

1. 基于内存的计算

   当我们把数据存储在 RDD 中后，数据就会保存在内存中，这样做可以极大地提高性能。

2. Lazy Evaluation Question

   Spark Lazy Evaluation 表示在 RDD 中的数据不会被随时评估。一般情况下，只有触发 action 导致改变后或者计算被执行时才会执行评估。因此，它限制了它必须做的多少工作。 

3. Fault Tolerance Question

   如果任何工作节点发送故障，通过使用 lineage 操作，可以从原始分区中重新计算丢失的 RDD 分区，基于此可以很容易的恢复丢失的数据。

4. 不可变

   一旦我们创建 RDD 之后，就不能对它进行修改，但是我们可以通过转换操作创建新的 RDD。至此，通过不可变性来达到了一致性。

5. 持久化

   我们可以在内存中存储经常需要使用的 RDD。这样便保证了我们可以在不去硬盘的情况下直接检索他们。我们还可以对相同的数据执行多个操作。具体是通过调用 `cache()` 或者 `persist()` 函数将数据保存在内存中。

6. 分区

   RDD Partition 是逻辑的记录集，用于在集群中分布跨节点的数据。逻辑上区分仅仅是为了处理数据方便，在物理层是没有真正进行区分的。因为分区的特性，为并行性提供了可能。

7. 并行性

   RDD 会在集群上并行的处理数据。

8. Location-Stickiness

   为了计算 Partition，RDD 可以定义存放 RDD 的位置。DAGScheduler 会采取任务和数据尽可能近的方式来存放分区，以此保证可以更快的计算。

9. 粗粒度的操作

   我们将粗粒度的转换操作应用在完整的数据集上，而不是应用在 RDD 数据集中的某个元素上。

10. Typed

    Spark RDD 存在几种类型。如：RDD [int], RDD [long], RDD [string].

11. 数量限制

    Spark RDD 的数量是没有限制的。基本上限制主要来自于磁盘和内存的大小。

### Spark Streaming

数据以一种无序序列持续到来的过程称为数据流。为了进一步处理这些数据，Streaming 将持续的数据流分离成离散单元。Streaming 对加载和分析流数据具有很低的延迟性。在 2013 年 Spark API Streaming 添加到 Apache Spark 项目中，对实时流处理保证了很好的扩展性，容错性和高吞吐量。Streaming 对很多的数据源是支持的。例如 Kafka、Apache Flume、Amazon Kinesis 或者 TCP sockets. 在操作这些数据时，我们可以使用高级的算法进行处理如 map、reduce、join 和 window 等。

####  Spark Streaming 内部工作情况

当实时的数据到来时，会被 Spark Streaming 分成多个批次。这些批次被 Spark 引擎进行处理，形成最终的结果流。

#### Discretized Stream (DStream)

Apache Spark Discretized Stream 是 Spark Streaming 的抽象。我们称它为 Spark DStream，表示数据流被分割成小的批次。DStream 是由 **Spark RDD** 构建的，这就意味着，它可以同 Spark 的其他 Component 无缝的整合。如何 Spark MLib 和 Spark SQL 等。

## Spark 的特性

下面是对 Spark 特性的介绍：
![Spark 特性](https://data-flair.training/blogs/wp-content/uploads/sites/2/2018/01/Features-of-Apache-Spark-01.jpg)

1. 快速加载

   Spark 提供了很高的数据加载速度。相比于 hadoop 在内存和硬盘的速度提高了 100 倍和 10 倍。为了保证这样的速度，是通过减少对硬盘的读写数量而实现的。

2. Dynamic in Nature

   Spark 是开发并行的应用称为可能，它提供了 80 个高级操作符。

3. 基于内存的计算

   由于在内存中处理的原因，使得处理速度进一步提高。

4. 重用性

   在批量处理，和将流连接到历史数据的过程中很容易的复用 Spark 代码，还可以在流状态上使用运行 ad-query.

5. 容错性

   Spark 的容错性主要通过 Spark 的抽象 RDD 来保证。RDD 被设计用于处理在集群中任何 worker 节点的失败。由于 RDD 的这个特性，可以保证数据丢失被减少到 0。

6. 实时流处理

   相比于 Hadoop 来说，Spark Streaming 提供了对实时流处理的功能。

7. Lazy Evaluation in Spark

   我们对 RDD 使用的所有转换操作都是 Lazy Evaluation，也就是说所有的操作都不会立马给出结果，而是从现有的 RDD 中生成一个新的 RDD。由于这个原因，可以让系统更有效率的执行。

8. 支持多个编程语言

   Spark 支持多个语言，包括 Java，R，Scala 和 Python。它克服了 Hadoop 的限制，只能用 Hadoop 进行构建应用。

9. 支持复杂的分析

   在 Spark 中存在着一系列工具以应对不同的复杂的分析，如交互式的流数据，附加 map 和 reduce 的机器学习。

10. 和 Hadoop 整合

    Spark 是非常灵活的，可以独立的运行，也可以在 Hadoop 中的 Yarn Cluster Manager 上运行。同时还可以在 Hadoop 的文件系统上读写数据。

11. Spark GraphX

    在 Spark 中提供了对图进行并行计算的组件 GraphX，它通过图算法和构建器的集合简化了图分析的任务。

12. Cost Efficient

    对于 Hadoop 中的大数据问题，是在备份期间需要大量的存储和大型的数据分析中心。 Spark 编程成为一种经济高效的解决方案。

## Spark 的限制

 Spark 也着很多的限制：

 ![Spark 限制](https://data-flair.training/blogs/wp-content/uploads/sites/2/2018/02/Limitations-of-Apache-Spark-01.jpg)

1. 不支持实时的流处理过程

   Spark 是接近实时流处理。换句话说，由于微批量进程在 Spark Streaming 中运行，我们不能说 Spark 是完全的实时流处理平台。

2. 小文件的问题

   在 RDD 中，每一个文件是一个小的分区。这就意味着，在 RDD 中存在着许多微小的分区。如果想要改善处理效率，RDD 应该被重新分区成一些可以管理的格式，这就需要在网络上广泛的通讯。

3. 没有文件管理系统

   依赖于其他平台 如 Hadoop 和其他云平台。

4. 价格高

   我们希望可以经济高效的处理大数据，但就 Spark 而言比较昂贵。由于 Spark 将数据保存在内存中，因此存储器消耗会非常高，我们需要大量的 RAM 来当做运行内存。

5. 算法的数量较少

   Spark MLib 里存在的算法较少。

6. 手动优化

   Spark 作业必须手动去优化去满足特定的数据集要求，并且为了保证 Spark 中分区和缓存是正确的，必须采用硬编码的形式。

7. 迭代处理

   在批量作业里的数据都是迭代的，每一次迭代调度和执行时分开的。

8. 延迟性

   相比于 Filnk，Apache Spark 有更高的延迟性。

9. 窗口标准

   Spark 仅支持基于时间的窗口标准，而不支持记录的窗口标准。

## Spark 的应用场景

Spark 在很多行业中有着应用场景：

![Spark 应用场景](https://data-flair.training/blogs/wp-content/uploads/sites/2/2018/01/Spark-Use-Cases-01-1.jpg)

1. 在金融业的应用场景

   许多银行在使用 Spark，它帮助访问和分析银行部门内的许多参数，如电子邮件、社交档案、电话记录、论坛等。

2. 在电子商务的应用场景

   Spark 帮助获取实时的交易信息，而且进一步将这些信息传递给流聚类算法。

3. 在媒体和娱乐业

   使用 Spark 来实时识别游戏事件中的模式。

4. 在旅游行业

   Spark 通过加速个性化的推荐帮助用户计划一个完美的旅行路线。

